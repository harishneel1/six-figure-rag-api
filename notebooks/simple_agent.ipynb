{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15de4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to Python path so we can import src\n",
    "# Get current working directory\n",
    "cwd = Path().resolve()\n",
    "\n",
    "# If we're in the notebooks directory, go up one level\n",
    "# Otherwise, assume we're already in the project root\n",
    "if cwd.name == \"notebooks\":\n",
    "\tproject_root = cwd.parent\n",
    "else:\n",
    "\tproject_root = cwd\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "\tsys.path.insert(0, str(project_root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a8b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from src.rag.retrieval.index import retrieve_context\n",
    "from src.rag.retrieval.utils import prepare_prompt_and_invoke_llm\n",
    "from src.config.index import appConfig\n",
    "import json\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "from typing import Any, List, Dict\n",
    "from typing_extensions import Annotated\n",
    "from langgraph.types import Command\n",
    "from langchain_core.tools.base import InjectedToolCallId\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3fcbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom agent state that extends the MessagesState to store citations\n",
    "class CustomAgentState(MessagesState):\n",
    "\t\"\"\"Extended agent state with citations tracking\"\"\"\n",
    "\t# citations will accumulate across tool calls\n",
    "\tcitations: Annotated[List[Dict[str, Any]], lambda x, y: x + y] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1459d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factory function to create RAG tool with project_id bound\n",
    "def create_rag_tool(project_id: str):\n",
    "\t\"\"\"Create a RAG search tool bound to a specific project\"\"\"\n",
    "\t\n",
    "\t@tool\n",
    "\tdef rag_search(\n",
    "\t\tquery: str,\n",
    "\t\ttool_call_id: Annotated[str, InjectedToolCallId],\n",
    "\t) -> Command:\n",
    "\t\t\"\"\"\n",
    "\t\tSearch through project documents using RAG (Retrieval-Augmented Generation).\n",
    "\t\tThis tool retrieves relevant context from the current project's documents based on the query.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tquery: The search query or question to find relevant information\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tA formatted string containing the retrieved context and answer based on the documents\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\t# Retrieve context using the existing RAG pipeline\n",
    "\t\t\ttexts, images, tables, citations = retrieve_context(project_id, query)\n",
    "\t\t\t\n",
    "\t\t\t# If no context found, return a message\n",
    "\t\t\tif not texts and not images and not tables:\n",
    "\t\t\t\treturn Command(\n",
    "\t\t\t\t\tupdate={\n",
    "\t\t\t\t\t\t\"messages\": [\n",
    "\t\t\t\t\t\t\tToolMessage(\n",
    "\t\t\t\t\t\t\t\t\"No relevant information found in the project documents for this query.\",\n",
    "\t\t\t\t\t\t\t\ttool_call_id=tool_call_id\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t]\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t# Prepare the response using the existing LLM preparation function\n",
    "\t\t\tresponse = prepare_prompt_and_invoke_llm(\n",
    "\t\t\t\tuser_query=query,\n",
    "\t\t\t\ttexts=texts,\n",
    "\t\t\t\timages=images,\n",
    "\t\t\t\ttables=tables\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\treturn Command(\n",
    "                update={\n",
    "                    # Update message history\n",
    "                    \"messages\": [\n",
    "                        ToolMessage(\n",
    "                            content=response,\n",
    "                            tool_call_id=tool_call_id\n",
    "                        )\n",
    "                    ],\n",
    "                    # Update citations in state - these accumulate!\n",
    "                    \"citations\": citations\n",
    "                }\n",
    "            )\t\t\n",
    "\t\texcept Exception as e:\n",
    "\t\t\treturn Command(update={\n",
    "                    \"messages\": [\n",
    "                        ToolMessage(\n",
    "                            f\"Error retrieving information: {str(e)}\",\n",
    "                            tool_call_id=tool_call_id\n",
    "                        )\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "\n",
    "\treturn rag_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03cadee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with project-specific RAG tool\n",
    "def create_rag_agent(project_id: str, model: str = \"gpt-4o\"):\n",
    "\t\"\"\"Create an agent with RAG tool for a specific project\"\"\"\n",
    "\t\n",
    "\t# Create tools list with project-specific RAG tool\n",
    "\ttools = [create_rag_tool(project_id)]\n",
    "\t\n",
    "\t# Define the system prompt\n",
    "\tsystem_prompt = \"\"\"You are a helpful AI assistant with access to a RAG (Retrieval-Augmented Generation) tool that searches project-specific documents.\n",
    "\n",
    "For every user question:\n",
    "\n",
    "1. Do not assume any question is purely conceptual or general.  \n",
    "2. Use the `rag_search` tool immediately with a clear and relevant query derived from the userâ€™s question.  \n",
    "3. Carefully review the retrieved documents and base your entire answer on the RAG results.  \n",
    "4. If the retrieved information fully answers the userâ€™s question, respond clearly and completely using that information.  \n",
    "5. If the retrieved information is insufficient or incomplete, explicitly state that and provide helpful suggestions or guidance based on what you found.  \n",
    "6. Always present answers in a clear, well-structured, and conversational manner.\n",
    "\n",
    "**Never answer without first querying the RAG tool. This ensures every response is grounded in project-specific context and documentation.**\n",
    "\"\"\"\n",
    "\t\n",
    "\t# Create the agent graph\n",
    "\tagent = create_agent(\n",
    "\t\tmodel=model,\n",
    "\t\ttools=tools,\n",
    "\t\tsystem_prompt=system_prompt,\n",
    "\t\tstate_schema=CustomAgentState\n",
    "\t)\n",
    "\t\n",
    "\treturn agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d165469",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"6d090d75-7c7c-428c-bba8-258cf3f45d2d\"\n",
    "rag_agent = create_rag_agent(project_id=project_id, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d58580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector search resulted in: 3 chunks\n",
      "ðŸ¤– Invoking LLM with 2 messages (3 texts, 0 tables, 0 images)...\n",
      "{'messages': [HumanMessage(content='What are the two types of sleep?', additional_kwargs={}, response_metadata={}, id='86c8d3b8-bc56-4d80-9e20-93137190115e'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 286, 'total_tokens': 303, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ed643dde95', 'id': 'chatcmpl-Cef9KlffZxqobXBI3WI0LHLAY5TWn', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--4b045f9f-37cb-4978-915f-30d11beb30e6-0', tool_calls=[{'name': 'rag_search', 'args': {'query': 'types of sleep'}, 'id': 'call_EYzUAlv6ZllaZXwNOBnB88m6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 286, 'output_tokens': 17, 'total_tokens': 303, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Sleep consists of two main types: non-REM (NREM) sleep and REM (rapid eye movement) sleep. NREM sleep has three stages, progressing from light to deep sleep, with Stage 3 NREM, also called slow-wave sleep, being the deepest stage. REM sleep is characterized by rapid eye movements, vivid dreams, temporary muscle paralysis, and brain activity similar to wakefulness.', name='rag_search', id='f3e840a6-718b-4ae7-85b5-457a91444046', tool_call_id='call_EYzUAlv6ZllaZXwNOBnB88m6'), AIMessage(content=\"The two main types of sleep are non-REM (NREM) sleep and REM (rapid eye movement) sleep.\\n\\n1. **Non-REM (NREM) Sleep**: This type of sleep is further divided into three stages, ranging from light to deep sleep. The deepest stage, called Stage 3 NREM or slow-wave sleep, is crucial for restorative processes.\\n\\n2. **REM Sleep**: During REM sleep, rapid eye movements occur, and this stage is often associated with vivid dreams. It is also characterized by temporary muscle paralysis and brain activity that is similar to wakefulness.\\n\\nThese cycles alternate throughout a typical night's sleep.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 391, 'total_tokens': 521, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ed643dde95', 'id': 'chatcmpl-Cef9NUp4cHNYSHPr5UbaHupiq60Mi', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--33fbc37a-3cc4-40b6-8d62-a5ced35fec7c-0', usage_metadata={'input_tokens': 391, 'output_tokens': 130, 'total_tokens': 521, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'citations': [{'chunk_id': '8559a311-5f35-4588-9b04-d8776b5f8dcf', 'document_id': 'c60e660f-7a2d-492a-aa0f-22b4e332cae9', 'filename': 'neuroscience.txt', 'page': 6}, {'chunk_id': 'f0c2d244-0880-4d6c-9fcc-aa7c122dcb53', 'document_id': 'c60e660f-7a2d-492a-aa0f-22b4e332cae9', 'filename': 'neuroscience.txt', 'page': 5}, {'chunk_id': 'd741ac19-c537-4fbf-a919-217560757003', 'document_id': 'c60e660f-7a2d-492a-aa0f-22b4e332cae9', 'filename': 'neuroscience.txt', 'page': 7}]}\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [{\"role\": \"user\", \"content\": \"What are the two types of sleep?\"}]}\n",
    "\n",
    "result = rag_agent.invoke(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5de74e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What are the two types of sleep?', additional_kwargs={}, response_metadata={}, id='86c8d3b8-bc56-4d80-9e20-93137190115e'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 286, 'total_tokens': 303, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ed643dde95', 'id': 'chatcmpl-Cef9KlffZxqobXBI3WI0LHLAY5TWn', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--4b045f9f-37cb-4978-915f-30d11beb30e6-0', tool_calls=[{'name': 'rag_search', 'args': {'query': 'types of sleep'}, 'id': 'call_EYzUAlv6ZllaZXwNOBnB88m6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 286, 'output_tokens': 17, 'total_tokens': 303, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='Sleep consists of two main types: non-REM (NREM) sleep and REM (rapid eye movement) sleep. NREM sleep has three stages, progressing from light to deep sleep, with Stage 3 NREM, also called slow-wave sleep, being the deepest stage. REM sleep is characterized by rapid eye movements, vivid dreams, temporary muscle paralysis, and brain activity similar to wakefulness.', name='rag_search', id='f3e840a6-718b-4ae7-85b5-457a91444046', tool_call_id='call_EYzUAlv6ZllaZXwNOBnB88m6'),\n",
       "  AIMessage(content=\"The two main types of sleep are non-REM (NREM) sleep and REM (rapid eye movement) sleep.\\n\\n1. **Non-REM (NREM) Sleep**: This type of sleep is further divided into three stages, ranging from light to deep sleep. The deepest stage, called Stage 3 NREM or slow-wave sleep, is crucial for restorative processes.\\n\\n2. **REM Sleep**: During REM sleep, rapid eye movements occur, and this stage is often associated with vivid dreams. It is also characterized by temporary muscle paralysis and brain activity that is similar to wakefulness.\\n\\nThese cycles alternate throughout a typical night's sleep.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 391, 'total_tokens': 521, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ed643dde95', 'id': 'chatcmpl-Cef9NUp4cHNYSHPr5UbaHupiq60Mi', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--33fbc37a-3cc4-40b6-8d62-a5ced35fec7c-0', usage_metadata={'input_tokens': 391, 'output_tokens': 130, 'total_tokens': 521, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'citations': [{'chunk_id': '8559a311-5f35-4588-9b04-d8776b5f8dcf',\n",
       "   'document_id': 'c60e660f-7a2d-492a-aa0f-22b4e332cae9',\n",
       "   'filename': 'neuroscience.txt',\n",
       "   'page': 6},\n",
       "  {'chunk_id': 'f0c2d244-0880-4d6c-9fcc-aa7c122dcb53',\n",
       "   'document_id': 'c60e660f-7a2d-492a-aa0f-22b4e332cae9',\n",
       "   'filename': 'neuroscience.txt',\n",
       "   'page': 5},\n",
       "  {'chunk_id': 'd741ac19-c537-4fbf-a919-217560757003',\n",
       "   'document_id': 'c60e660f-7a2d-492a-aa0f-22b4e332cae9',\n",
       "   'filename': 'neuroscience.txt',\n",
       "   'page': 7}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
